{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_zfYWMZASNj"
   },
   "source": [
    "# Week 13 Exercise - Transformers\n",
    "\n",
    "In this notebook, we will explore the creation of a Transformer Network for English to French translation.  Note that **Transformers are resource intensive and hard to train.** You will want to run these notebooks on a machine equipped with a GPU or on [Google Colab](http://colab.research.google.com).\n",
    "\n",
    "To begin, let's import a corpus of paired English and French text.  Additionally, we'll tokenize the words (i.e. create a dictionary for each vocabulary associating every word with an integer index).  There is no need to modify this cell, but have a look at what is contained in fr_to_ix (for example) and in enlines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "DGaI25ygASNn",
    "outputId": "b5ff2933-19b6-4d6a-92a1-651772391d39"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "with open('./french.txt', encoding=\"utf-8\") as file:\n",
    "    frvocab = file.read().lower()\n",
    "    frvocab = ''.join([i if ord(i) < 128 else ' ' for i in frvocab])\n",
    "    frlines = frvocab.split('\\n')\n",
    "frlines = [re.sub(r'[^\\w\\s\\']','',i).split() for i in frlines]\n",
    "frvocab = set(re.sub(r'[^\\w\\s\\']','',frvocab).replace('\\n',' ').split(' '))\n",
    "\n",
    "with open('./english.txt', encoding=\"utf-8\") as file:\n",
    "    envocab = file.read().lower()\n",
    "    envocab = ''.join([i if ord(i) < 128 else '' for i in envocab])\n",
    "    enlines = envocab.split('\\n')\n",
    "enlines = [re.sub(r'[^\\w\\s]','',i).split() for i in enlines]\n",
    "envocab = set(re.sub(r'[^\\w\\s]','',envocab).replace('\\n',' ').strip().split(' '))\n",
    "envocab.add('<pad>')\n",
    "envocab.add('<start>')\n",
    "envocab.add('<eos>')\n",
    "frvocab.add('<pad>')\n",
    "frvocab.add('<start>')\n",
    "frvocab.add('<eos>')\n",
    "fr_to_ix = {word: i for i, word in enumerate(frvocab)}\n",
    "en_to_ix = {word: i for i, word in enumerate(envocab)}\n",
    "ix_to_fr = {fr_to_ix[word]:word for word in frvocab}\n",
    "ix_to_en = {en_to_ix[word]:word for word in envocab}\n",
    "enmax = 0\n",
    "frmax = 16\n",
    "\n",
    "for i,w in enumerate(enlines):\n",
    "    temp = len(w)\n",
    "    if temp > enmax:\n",
    "        enmax = temp\n",
    "\n",
    "for i,w in enumerate(frlines):\n",
    "    temp = len(w)\n",
    "    if temp > frmax:\n",
    "        frmax = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oblZwXf5ASN3"
   },
   "source": [
    "Next we'll create a handful of helper functions that do things like\n",
    " - Tokenize an english string, run it through the transformer producing predictions, then convert back to a french string\n",
    " - Compare predicted and target output\n",
    " - Mask a string\n",
    " - Load paired english/french sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABGWJB3eASN8"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    # Read in an english string\n",
    "    line = re.sub(r'[^\\w\\s]','',sentence).split()\n",
    "    # tokenize/pad for consistent sequence length\n",
    "    line = F.pad(torch.tensor([en_to_ix[w.lower()] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).unsqueeze(0).to(device)\n",
    "    # Create an array to hold the French sentence\n",
    "    target = torch.Tensor(1,frmax-1)\n",
    "    target = target.new_full((1,frmax-1),fr_to_ix['<pad>']).long().to(device)\n",
    "    # Start sentence with a <start> character\n",
    "    target[0,0] = fr_to_ix['<start>']\n",
    "    \n",
    "    src,trg = mask(line,target)\n",
    "    encoding = model.encode(line,src)\n",
    "    K,V = model.create_dec_KV(encoding)\n",
    "    for i in range(1,frmax-1):\n",
    "        test2 = model.decode(target,K,V,src,trg)\n",
    "        lastout = test2[0,i-1].argmax()\n",
    "        if lastout.item() == fr_to_ix['<eos>']:\n",
    "            break\n",
    "        target[0,i] = lastout\n",
    "        src,trg = mask(line,target)\n",
    "    translation = test2.argmax(2).squeeze(0)\n",
    "    translation_string = ''\n",
    "    for w in translation:\n",
    "        if ix_to_fr[w.item()] == '<eos>':\n",
    "            break\n",
    "        translation_string += ix_to_fr[w.item()] + ' '\n",
    "    return translation_string.strip()\n",
    "\n",
    "def compareoutput(preds,targetlist,loc=None):\n",
    "    # Compare model predictions with true translation\n",
    "    if loc is None:\n",
    "        loc = np.random.randint(len(preds))\n",
    "    predstr = ''\n",
    "    labelstr = ''\n",
    "    for i in range(len(preds[loc][0])):\n",
    "        if ix_to_fr[targetlist[loc][i+1].item()] == '<eos>':\n",
    "            break\n",
    "        predstr += ' '+ ix_to_fr[preds[loc][0][i].item()]\n",
    "        labelstr += ' ' + ix_to_fr[targetlist[loc][i+1].item()]\n",
    "    print(\"\\tOutput:\", predstr)\n",
    "    print(\"\\tTarget:\",labelstr)\n",
    "    \n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Create a positional encoding generator\n",
    "    def __init__(self, d_model, max_seq_len = 58):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            for i in range(1,d_model,2):\n",
    "                pe[pos, i] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "        pe = pe\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:seq_len], \\\n",
    "        requires_grad=False).to(device)\n",
    "        return x\n",
    "\n",
    "def mask(input_seq,target_seq):\n",
    "    input_msk = (input_seq != en_to_ix['<pad>']).unsqueeze(1)\n",
    "    target_msk = (target_seq != fr_to_ix['<pad>']).unsqueeze(1)\n",
    "    size = target_seq.size(1) # get seq_len for matrix\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size)),k=1)\n",
    "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask).to(device) == 0)\n",
    "    target_msk = target_msk & nopeak_mask\n",
    "    return input_msk,target_msk\n",
    "\n",
    "class custdata(Dataset):\n",
    "    # Create a custom dataset object to serve up paired english and french lines\n",
    "    def __init__(self,enlines,frlines):\n",
    "        self.data_len = len(enlines) \n",
    "        self.data = [F.pad(torch.tensor([en_to_ix[w] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).to(device) for line in enlines]\n",
    "        self.labels = []\n",
    "        for line in frlines:\n",
    "            line = ['<start>',*line,'<eos>']\n",
    "            self.labels.append(F.pad(torch.tensor([fr_to_ix[w] for w in line]),(0,frmax-len(line)),value = fr_to_ix['<pad>']).to(device))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i],self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFCn7vfkASOF"
   },
   "source": [
    "# Attention\n",
    "The first task is to code a self-attention mechanism, which corresponds to implementing Eq. 1 in Vaswani.\n",
    "#### http://jalammar.github.io/illustrated-transformer/ is a great reference for most of the programming in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MC5U_PgASOI"
   },
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR Q, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wk = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR K, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wv = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR V, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scaler = np.sqrt(enc_dim)\n",
    "    \n",
    "    def QKV(self,x):\n",
    "        Q = self.wq(x)  #### TODO#### CALCULATE Q\n",
    "        K = self.wk(x)  #### TODO#### CALCULATE K\n",
    "        V = self.wv(x)  #### TODO#### CALCULATE V\n",
    "        return Q,K,V\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        # scores are the stuff that goes inside the softmax\n",
    "        scores = (Q @ K.T.permute(2,0,1)) / self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores @ V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        Q,K,V = self.QKV(x)\n",
    "        return self.score(Q,K,V,mask)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "657JDHJiASOR"
   },
   "source": [
    "Next, we need to produce the \"special\" attention mechanism that takes keys and values from the encoder, but queries from the decoder.  This is very similar to the self-attention mechanism, except that there should be two inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO1ixq6WASOS"
   },
   "outputs": [],
   "source": [
    "class encdec_attention(nn.Module):\n",
    "    def __init__(self,dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.wk = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.wv = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.scaler = np.sqrt(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def Q(self,x):\n",
    "        return self.wq(x)\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        scores = (Q @ K.T.permute(2,0,1)) / self.scaler  #### TODO #### SAME AS ABOVE\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores @ V  #### TODO #### SAME AS ABOVE\n",
    "    \n",
    "    def forward(self,x,K,V,mask):\n",
    "        # DB Note: I'm not sure that this signature is right.  Seems like we should be taking x from the\n",
    "        # decoder, as well as another argument (call it y?) from the encoder, then producing K,V,Q internally,\n",
    "        # just like in the self-attention scheme.  Otherwise, how are wk and wv being used here?  it looks like \n",
    "        # these parameters have been shifted over to the Transformer module's create_dec_KV method.\n",
    "        Q = self.Q(x)\n",
    "        out = self.score(Q,K,V,mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPPcqLcUASOa"
   },
   "source": [
    "# Encoder and Decoder\n",
    "With the attention mechanisms coded, now we need to create encoder and decoder models. These correspond to the things inside the boxes in Figure 1 of Vaswani.  \n",
    "#### Fill in the forward passes of the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHb5vge7ASOc"
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        \n",
    "        self.attention = self_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):  #### TODO #### SET UP FORWARD PASS OF ENCODER\n",
    "        atten = self.attention(x, mask)\n",
    "        if self.dim != self.enc_dim: ### DONT TOUCH, THIS IS TO HELP WITH THE RESIDUAL CONNECTION ###\n",
    "            x = self.residual(x)\n",
    "        x = self.norm1(x + atten)\n",
    "        x = self.linear(x)\n",
    "        return self.norm2(x)\n",
    "     \n",
    "        \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = self_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,enc_mask,dec_mask):#### TODO #### SET UP FORWARD PASS OF DECODER\n",
    "        x = self.attention(x, dec_mask)\n",
    "        x = self.norm1(x)\n",
    "        x = self.EDattention(x, k, v, enc_mask)\n",
    "        x = self.norm2(x)\n",
    "        x = self.linear(x)\n",
    "        return self.norm3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBHUIU-HASOm"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Build the transformer itself by hooking together encoders and decoders.  Note the word embedding layers that we are going to learn.  \n",
    "\n",
    "#### Add encoders and decoders to transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eUR7y_ZASOo"
   },
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        self.encoders = []\n",
    "    \n",
    "        self.encoders.append( encoder(dim, encoder_dim, enc_vocab_size) ) #### TODO #### ADD DESIRED # OF ENCODERS TO SELF.ENCODERS\n",
    "        \n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        \n",
    "        self.decoders = []\n",
    "        \n",
    "        self.decoders.append( decoder(encoder_dim, encoder_dim, dec_vocab_size) ) #### TODO #### ADD DESIRED # OF DECODERS TO SELF.DECODERS\n",
    "\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        \n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgNROk5WASOw"
   },
   "source": [
    "# Train the network.\n",
    "\n",
    "##### This will be slow to train and require a lot of resources. You can reduce the batch_size to lower the vram requirement, you can reduce \n",
    "##### the run time by lowering number_of_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvlqFu36ASOx"
   },
   "outputs": [],
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLiY-euGASO3"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36VQ5YDAASO-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "NUMBER_OF_LINES = 2\n",
    "\n",
    "train = custdata(enlines[:NUMBER_OF_LINES],frlines[:NUMBER_OF_LINES])\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val = custdata(enlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000],frlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000])\n",
    "valloader = torch.utils.data.DataLoader(dataset=val, batch_size=1, shuffle=True, drop_last=False)\n",
    "test = custdata(enlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000],frlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000])\n",
    "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IA3ev6NlASPF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 10.011906623840332\n",
      "\tOutput:  essayait essayait\n",
      "\tTarget:  au feu\n",
      "Epoch: 2  loss: 8.468424797058105\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  elle est morte\n",
      "Epoch: 3  loss: 7.547512531280518\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  asseyezvous ici\n",
      "Epoch: 4  loss: 6.608973503112793\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  estce toi\n",
      "Epoch: 5  loss: 5.826406955718994\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  restez allong s immobiles\n",
      "Epoch: 6  loss: 4.986212253570557\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  gardele\n",
      "Epoch: 7  loss: 4.018810749053955\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  je comprends\n",
      "Epoch: 8  loss: 2.962115526199341\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  je paierai\n",
      "Epoch: 9  loss: 1.9250057935714722\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  j'ai gagn\n",
      "Epoch: 10  loss: 1.0973504781723022\n",
      "\tOutput:  d'innombrables d'innombrables\n",
      "\tTarget:  nous gagnerons\n",
      "Epoch: 11  loss: 0.6054789423942566\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  aide tom\n",
      "Epoch: 12  loss: 0.3560470640659332\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  reste en arri re\n",
      "Epoch: 13  loss: 0.3464266359806061\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  sortez\n",
      "Epoch: 14  loss: 0.33716943860054016\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  l vetoi\n",
      "Epoch: 15  loss: 0.3258271813392639\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  apportez du vin\n",
      "Epoch: 16  loss: 0.3164551556110382\n",
      "\tOutput:  correcte correcte\n",
      "\tTarget:  nous essayerons\n",
      "Epoch: 17  loss: 0.30512383580207825\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  ils nageaient\n",
      "Epoch: 18  loss: 0.29919785261154175\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  tesvous lev e\n",
      "Epoch: 19  loss: 0.28693482279777527\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  allez doucement\n",
      "Epoch: 20  loss: 0.2849518954753876\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  laissezmoi entrer\n",
      "Epoch: 21  loss: 0.28087136149406433\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  je l'ai fait\n",
      "Epoch: 22  loss: 0.27539727091789246\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  nous avons oubli\n",
      "Epoch: 23  loss: 0.27048009634017944\n",
      "\tOutput:  haletaient\n",
      "\tTarget:  vraiment\n",
      "Epoch: 24  loss: 0.27299296855926514\n",
      "\tOutput:  m'adapte m'adapte\n",
      "\tTarget:  c'est perdu\n",
      "Epoch: 25  loss: 0.2707909047603607\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  j'ai du pot\n",
      "Epoch: 26  loss: 0.26679906249046326\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  je suis dur cuire\n",
      "Epoch: 27  loss: 0.26910150051116943\n",
      "\tOutput:  homologues homologues homologues\n",
      "\tTarget:  c'est pas possible\n",
      "Epoch: 28  loss: 0.2666018009185791\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  va t'en\n",
      "Epoch: 29  loss: 0.2696802616119385\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  je suis grande\n",
      "Epoch: 30  loss: 0.2681148648262024\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  qui est tomb\n",
      "Epoch: 31  loss: 0.26631835103034973\n",
      "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
      "\tTarget:  il me faut m'en aller\n",
      "Epoch: 32  loss: 0.26961833238601685\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  d tendstoi\n",
      "Epoch: 33  loss: 0.2684692442417145\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  entrez\n",
      "Epoch: 34  loss: 0.2700534760951996\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  va te faire foutre\n",
      "Epoch: 35  loss: 0.2679598927497864\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  je suis parti\n",
      "Epoch: 36  loss: 0.26726600527763367\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  restez allong e immobile\n",
      "Epoch: 37  loss: 0.2674499452114105\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  je suis paresseux\n",
      "Epoch: 38  loss: 0.26692554354667664\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  tom marche\n",
      "Epoch: 39  loss: 0.2683902382850647\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  je suis pr t\n",
      "Epoch: 40  loss: 0.2661706507205963\n",
      "\tOutput:  l'ingr l'ingr l'ingr l'ingr\n",
      "\tTarget:  c'est de la nourriture\n",
      "Epoch: 41  loss: 0.26536279916763306\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  crivezmoi\n",
      "Epoch: 42  loss: 0.2661800980567932\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  viens\n",
      "Epoch: 43  loss: 0.2662958800792694\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  retiretoi\n",
      "Epoch: 44  loss: 0.26549774408340454\n",
      "\tOutput:  <pad> <pad> <pad>\n",
      "\tTarget:  je suis gras\n",
      "Epoch: 45  loss: 0.26557061076164246\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  asseyezvous\n",
      "Epoch: 46  loss: 0.26559457182884216\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  viens seule\n",
      "Epoch: 47  loss: 0.2677525579929352\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  laissenous\n",
      "Epoch: 48  loss: 0.26179054379463196\n",
      "\tOutput:  <pad> <pad> <pad> <pad>\n",
      "\tTarget:  je suis touch e\n",
      "Epoch: 49  loss: 0.26037418842315674\n",
      "\tOutput:  <pad>\n",
      "\tTarget:  j'abandonnerai\n",
      "Epoch: 50  loss: 0.26378655433654785\n",
      "\tOutput:  <pad> <pad>\n",
      "\tTarget:  soyez quitables\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zw_CG0QASPK"
   },
   "source": [
    "# Test your translator\n",
    "\n",
    "##### Unless you speak french you're going have to check it with google translate https://translate.google.com/\n",
    "##### I found it started doing alright once the loss got below 10 but this might take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw8-XQEtASPM"
   },
   "outputs": [],
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9z_zsUiASPV"
   },
   "source": [
    "#### Test it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6-kFpCyASPX"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "preds = []\n",
    "targetlist = []\n",
    "for j,(context, target) in enumerate(testloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        targetlist.append(targets)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        pred = F.softmax(output,2).argmax(2)\n",
    "        preds.append(pred)\n",
    "        correct = sum(pred[0][targets!=fr_to_ix['<pad>']]==targets[targets!=fr_to_ix['<pad>']]).item()/len(targets[targets!=fr_to_ix['<pad>']])\n",
    "        scores.append(correct)\n",
    "plt.plot(scores)\n",
    "print('Average # of words correct',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5B6ZDTMjASPc"
   },
   "source": [
    "# MULTI-HEAD ATTENTION (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOa0ZOzFASPd"
   },
   "outputs": [],
   "source": [
    "class multi_attention(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        \n",
    "        heads = []\n",
    "        for i in range(???): ### TODO ### CHOOSE THE # OF HEADS YOU WANT\n",
    "            heads.append(???) ### TODO ### ADD SELF_ATTENTION LAYERS TO HEADS\n",
    "        \n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        \n",
    "        self.linear = nn.Linear(???,encoder_dim) ### TODO ###\n",
    "    \n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        headoutputs = [layer(x,mask) for layer in self.heads]\n",
    "        headoutputs = torch.cat(headoutputs,dim=2)\n",
    "        return self.linear(headoutputs)\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.attention = multi_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        z = self.attention(x,mask)\n",
    "        if self.dim != self.enc_dim:\n",
    "            x = self.residual(x)\n",
    "        z = self.norm1(x+z)\n",
    "        z2 = self.linear(z)\n",
    "        return self.norm2(z+z2)\n",
    "     \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = multi_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,src,trg):\n",
    "        z = self.attention(x,trg)\n",
    "        z = self.norm1(z+x)\n",
    "        z2 = self.EDattention(z,k,v,src)\n",
    "        z2 = self.norm2(z2+z)\n",
    "        z3 = self.linear(z2)\n",
    "        return self.norm3(z3+z2)\n",
    "    \n",
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        self.encoders = []\n",
    "    \n",
    "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))   ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        \n",
    "        self.decoders = []\n",
    "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "        \n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFJz_fp-ASPl"
   },
   "outputs": [],
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeHXITQDASPv"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAlFnGpgASP1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrQ8CIIkASP6"
   },
   "source": [
    "# Test your  multi-head transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30Zhfoc_ASP7"
   },
   "outputs": [],
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3ye6qmhASQE"
   },
   "source": [
    "# QUESTION\n",
    "\n",
    "#### 1) Was the runtime of your multi-head transformer noticably longer than the single head one? What about the speed the loss decreased? If you had the time and resources to train it to a good spot, how did the translation quality compare to the single-headed transformer?\n",
    "\n",
    "#### 2)Try adding encoders and decoders to one of your transformers. Does having the extra layers improve performance? How does it affect runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHSJQX-pASQF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Attention_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
